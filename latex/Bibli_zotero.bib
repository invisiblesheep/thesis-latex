
@book{baeza-yates1999,
  title = {Modern Information Retrieval},
  author = {{Baeza-Yates}, Ricardo and {Ribeiro-Neto}, Berthier},
  year = {1999},
  volume = {463},
  publisher = {{ACM press New York}},
  file = {/Users/tuomas/Zotero/storage/8GFVVXXT/chap10.ps}
}

@article{ben-hur2001,
  title = {Support Vector Clustering},
  author = {{Ben-Hur}, Asa and Horn, David and Siegelmann, Hava T. and Vapnik, Vladimir},
  year = {2001},
  volume = {2},
  pages = {125--137},
  file = {/Users/tuomas/Zotero/storage/XNE9UE26/Ben-Hur et al. - 2001 - Support vector clustering.pdf;/Users/tuomas/Zotero/storage/HQRHHGZ6/horn01a.html},
  journal = {Journal of machine learning research},
  number = {Dec}
}

@article{bengio1994,
  title = {Learning Long-Term Dependencies with Gradient Descent Is Difficult},
  author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  year = {1994},
  volume = {5},
  pages = {157--166},
  publisher = {{IEEE}},
  file = {/Users/tuomas/Zotero/storage/3FY6YR67/Bengio et al_1994_Learning long-term dependencies with gradient descent is difficult.pdf},
  isbn = {1045-9227},
  journal = {IEEE transactions on neural networks},
  number = {2}
}

@article{bengio2003,
  title = {A {{Neural Probabilistic Language Model}}},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  year = {2003},
  pages = {19},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  file = {/Users/tuomas/Zotero/storage/SKEWLYTV/Bengio et al. - A Neural Probabilistic Language Model.pdf},
  language = {en}
}

@article{bojanowski2017,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  year = {2017},
  month = jun,
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character \$n\$-grams. A vector representation is associated to each character \$n\$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  archivePrefix = {arXiv},
  eprint = {1607.04606},
  eprinttype = {arxiv},
  file = {/Users/tuomas/Zotero/storage/RTCTLWHM/Bojanowski et al_2017_Enriching Word Vectors with Subword Information.pdf;/Users/tuomas/Zotero/storage/IXNT7CCN/1607.html},
  journal = {arXiv:1607.04606 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{cho2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder}}-{{Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  month = sep,
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  archivePrefix = {arXiv},
  eprint = {1406.1078},
  eprinttype = {arxiv},
  file = {/Users/tuomas/Zotero/storage/BEYZYU4L/Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf;/Users/tuomas/Zotero/storage/BJVP6CY6/1406.html},
  journal = {arXiv:1406.1078 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{clark2020,
  title = {Electra: {{Pre}}-Training Text Encoders as Discriminators Rather than Generators},
  author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
  year = {2020},
  file = {/Users/tuomas/Zotero/storage/987J8IHY/Clark et al_2020_Electra.pdf},
  journal = {arXiv preprint arXiv:2003.10555}
}

@article{collobert2011,
  title = {Natural Language Processing (Almost) from Scratch},
  author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
  year = {2011},
  volume = {12},
  pages = {2493--2537},
  file = {/Users/tuomas/Zotero/storage/T7L6EXUK/Collobert et al_2011_Natural language processing (almost) from scratch.pdf},
  journal = {Journal of machine learning research},
  number = {Aug}
}

@article{dai2015,
  title = {Semi-Supervised {{Sequence Learning}}},
  author = {Dai, Andrew M. and Le, Quoc V.},
  year = {2015},
  month = nov,
  abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
  archivePrefix = {arXiv},
  eprint = {1511.01432},
  eprinttype = {arxiv},
  file = {/Users/tuomas/Zotero/storage/PYL2A2PF/Dai_Le_2015_Semi-supervised Sequence Learning.pdf;/Users/tuomas/Zotero/storage/MR9WR3YZ/1511.html},
  journal = {arXiv:1511.01432 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{devlin2019,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archivePrefix = {arXiv},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  file = {/Users/tuomas/Zotero/storage/ASMXQK9U/Devlin et al_2019_BERT.pdf;/Users/tuomas/Zotero/storage/WJCQNVUA/1810.html},
  journal = {arXiv:1810.04805 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{gers2000,
  title = {Learning to {{Forget}}: {{Continual Prediction}} with {{LSTM}}},
  shorttitle = {Learning to {{Forget}}},
  author = {Gers, Felix A. and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  year = {2000},
  month = oct,
  volume = {12},
  pages = {2451--2471},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976600300015015},
  abstract = {Long Short-Term Memory (LSTM, Hochreiter \& Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow inde nitely and eventually cause the network to break down. Our remedy is a novel, adaptive \textbackslash{}forget gate" that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them in an elegant way.},
  file = {/Users/tuomas/Zotero/storage/7AP7ARFT/Gers et al. - 2000 - Learning to Forget Continual Prediction with LSTM.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {10}
}

@inproceedings{gers2000a,
  title = {Recurrent Nets That Time and Count},
  booktitle = {Proceedings of the {{IEEE}}-{{INNS}}-{{ENNS International Joint Conference}} on {{Neural Networks}}. {{IJCNN}} 2000. {{Neural Computing}}: {{New Challenges}} and {{Perspectives}} for the {{New Millennium}}},
  author = {Gers, Felix A. and Schmidhuber, J{\"u}rgen},
  year = {2000},
  volume = {3},
  pages = {189--194},
  publisher = {{IEEE}},
  file = {/Users/tuomas/Zotero/storage/DGH9SJDR/Gers_Schmidhuber_2000_Recurrent nets that time and count.pdf},
  isbn = {0-7695-0619-4}
}

@article{goldberg2014,
  title = {Word2vec {{Explained}}: Deriving {{Mikolov}} et al.'s Negative-Sampling Word-Embedding Method},
  author = {Goldberg, Yoav and Levy, Omer},
  year = {2014},
  file = {/Users/tuomas/Zotero/storage/IYMCWHTU/Goldberg_Levy_2014_word2vec Explained.pdf},
  journal = {arXiv preprint arXiv:1402.3722}
}

@inproceedings{he2016,
  title = {Deep Residual Learning for Image Recognition},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778},
  file = {/Users/tuomas/Zotero/storage/33DJANSG/He et al_2016_Deep residual learning for image recognition.pdf}
}

@article{hinton,
  title = {Learning {{Internal Representations}} by {{Error Propagation}}},
  author = {Hinton, E},
  pages = {49},
  file = {/Users/tuomas/Zotero/storage/GHJNGE7F/Hinton - Learning Internal Representations by Error Propaga.pdf},
  language = {en}
}

@article{hochreiter1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  volume = {9},
  pages = {1735--1780},
  publisher = {{MIT Press}},
  file = {/Users/tuomas/Zotero/storage/9NE2MFLK/Long Short-Term Memory.pdf},
  isbn = {0899-7667},
  journal = {Neural computation},
  number = {8}
}

@article{hotho,
  title = {A {{Brief Survey}} of {{Text Mining}}},
  author = {Hotho, Andreas and Nurnberger, Andreas and Paa{\ss}, Gerhard and Augustin, Sankt},
  pages = {37},
  abstract = {The enormous amount of information stored in unstructured texts cannot simply be used for further processing by computers, which typically handle text as simple sequences of character strings. Therefore, specific (pre-)processing methods and algorithms are required in order to extract useful patterns. Text mining refers generally to the process of extracting interesting information and knowledge from unstructured text. In this article, we discuss text mining as a young and interdisciplinary field in the intersection of the related areas information retrieval, machine learning, statistics, computational linguistics and especially data mining. We describe the main analysis tasks preprocessing, classification, clustering, information extraction and visualization. In addition, we briefly discuss a number of successful applications of text mining.},
  file = {/Users/tuomas/Zotero/storage/U4KCLSVT/Hotho et al. - A Brief Survey of Text Mining.pdf},
  language = {en}
}

@article{howard2018,
  title = {Universal {{Language Model Fine}}-Tuning for {{Text Classification}}},
  author = {Howard, Jeremy and Ruder, Sebastian},
  year = {2018},
  month = may,
  abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
  archivePrefix = {arXiv},
  eprint = {1801.06146},
  eprinttype = {arxiv},
  file = {/Users/tuomas/Zotero/storage/KQMHVFJS/Howard_Ruder_2018_Universal Language Model Fine-tuning for Text Classification.pdf;/Users/tuomas/Zotero/storage/3M3DMLL4/1801.html},
  journal = {arXiv:1801.06146 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{james2013,
  title = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  volume = {103},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-7138-7},
  file = {/Users/tuomas/Zotero/storage/FH922QRY/James et al. - 2013 - An Introduction to Statistical Learning.pdf},
  isbn = {978-1-4614-7137-0 978-1-4614-7138-7},
  language = {en},
  series = {Springer {{Texts}} in {{Statistics}}}
}

@article{joulin2016,
  title = {Bag of {{Tricks}} for {{Efficient Text Classification}}},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  year = {2016},
  month = aug,
  abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore\textasciitilde{}CPU, and classify half a million sentences among\textasciitilde{}312K classes in less than a minute.},
  archivePrefix = {arXiv},
  eprint = {1607.01759},
  eprinttype = {arxiv},
  file = {/Users/tuomas/Zotero/storage/XVSMYNIH/Joulin et al_2016_Bag of Tricks for Efficient Text Classification.pdf;/Users/tuomas/Zotero/storage/2I656GXM/1607.html},
  journal = {arXiv:1607.01759 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{kudo2018,
  title = {{{SentencePiece}}: {{A}} Simple and Language Independent Subword Tokenizer and Detokenizer for {{Neural Text Processing}}},
  shorttitle = {{{SentencePiece}}},
  author = {Kudo, Taku and Richardson, John},
  year = {2018},
  month = aug,
  abstract = {This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.},
  archivePrefix = {arXiv},
  eprint = {1808.06226},
  eprinttype = {arxiv},
  file = {/Users/tuomas/Zotero/storage/LY4YJAE7/Kudo and Richardson - 2018 - SentencePiece A simple and language independent s.pdf;/Users/tuomas/Zotero/storage/FV4F6RDY/1808.html},
  journal = {arXiv:1808.06226 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{lecun2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  volume = {521},
  pages = {436--444},
  publisher = {{Nature Publishing Group}},
  file = {/Users/tuomas/Zotero/storage/BIPF3UAW/LeCun et al_2015_Deep learning.pdf},
  isbn = {1476-4687},
  journal = {nature},
  number = {7553}
}

@article{levy2015,
  title = {Improving {{Distributional Similarity}} with {{Lessons Learned}} from {{Word}}                     {{Embeddings}}},
  author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
  year = {2015},
  month = dec,
  volume = {3},
  pages = {211--225},
  publisher = {{MIT Press}},
  doi = {10.1162/tacl_a_00134},
  abstract = {Recent trends suggest that neural-network-inspired word embedding models                     outperform traditional count-based distributional models on word similarity and                     analogy detection tasks. We reveal that much of the performance gains of word                     embeddings are due to certain system design choices and hyperparameter                     optimizations, rather than the embedding algorithms themselves. Furthermore, we                     show that these modifications can be transferred to traditional distributional                     models, yielding similar gains. In contrast to prior reports, we observe mostly                     local or insignificant performance differences between the methods, with no                     global advantage to any single approach over the others.},
  file = {/Users/tuomas/Zotero/storage/355EPKEE/Levy et al_2015_Improving Distributional Similarity with Lessons Learned from Word.pdf;/Users/tuomas/Zotero/storage/7IKVVW3R/tacl_a_00134.html},
  journal = {Transactions of the Association for Computational Linguistics}
}

@incollection{lewis1998,
  title = {Naive ({{Bayes}}) at Forty: {{The}} Independence Assumption in Information Retrieval},
  shorttitle = {Naive ({{Bayes}}) at Forty},
  booktitle = {Machine {{Learning}}: {{ECML}}-98},
  author = {Lewis, David D.},
  editor = {Carbonell, Jaime G. and Siekmann, J{\"o}rg and Goos, G. and Hartmanis, J. and {van Leeuwen}, J. and N{\'e}dellec, Claire and Rouveirol, C{\'e}line},
  year = {1998},
  volume = {1398},
  pages = {4--15},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0026666},
  file = {/Users/tuomas/Zotero/storage/3AN5KD9G/Lewis - 1998 - Naive (Bayes) at forty The independence assumptio.pdf},
  isbn = {978-3-540-64417-0 978-3-540-69781-7},
  language = {en},
  note = {Series Title: Lecture Notes in Computer Science}
}

@article{merity2017,
  title = {Regularizing and {{Optimizing LSTM Language Models}}},
  author = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  year = {2017},
  month = aug,
  abstract = {Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.},
  archivePrefix = {arXiv},
  eprint = {1708.02182},
  eprinttype = {arxiv},
  file = {/Users/tuomas/Zotero/storage/Y844SVXE/Merity et al_2017_Regularizing and Optimizing LSTM Language Models.pdf;/Users/tuomas/Zotero/storage/TMQTJWH4/1708.html},
  journal = {arXiv:1708.02182 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@incollection{mikolov2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  year = {2013},
  pages = {3111--3119},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/tuomas/Zotero/storage/3Z28B6HT/Mikolov et al_2013_Distributed Representations of Words and Phrases and their Compositionality.pdf;/Users/tuomas/Zotero/storage/2U2MNSLD/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.html}
}

@article{mikolov2013a,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archivePrefix = {arXiv},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  file = {/Users/tuomas/Zotero/storage/GGS34XCB/Mikolov et al_2013_Efficient Estimation of Word Representations in Vector Space.pdf;/Users/tuomas/Zotero/storage/G3GAK9Z6/1301.html},
  journal = {arXiv:1301.3781 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{mikolov2017,
  title = {Advances in {{Pre}}-{{Training Distributed Word Representations}}},
  author = {Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin, Armand},
  year = {2017},
  month = dec,
  abstract = {Many Natural Language Processing applications nowadays rely on pre-trained word representations estimated from large text corpora such as news collections, Wikipedia and Web Crawl. In this paper, we show how to train high-quality word vector representations by using a combination of known tricks that are however rarely used together. The main result of our work is the new set of publicly available pre-trained models that outperform the current state of the art by a large margin on a number of tasks.},
  archivePrefix = {arXiv},
  eprint = {1712.09405},
  eprinttype = {arxiv},
  file = {/Users/tuomas/Zotero/storage/THLBNM5P/Mikolov et al_2017_Advances in Pre-Training Distributed Word Representations.pdf;/Users/tuomas/Zotero/storage/QMJ9ZC49/1712.html},
  journal = {arXiv:1712.09405 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{pal1992,
  title = {Multilayer Perceptron, Fuzzy Sets, Classifiaction},
  author = {Pal, Sankar K. and Mitra, Sushmita},
  year = {1992},
  file = {/Users/tuomas/Zotero/storage/YBPX223G/Pal_Mitra_1992_Multilayer perceptron, fuzzy sets, classifiaction.pdf}
}

@inproceedings{pennington2014,
  title = {Glove: {{Global}} Vectors for Word Representation},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
  year = {2014},
  pages = {1532--1543},
  file = {/Users/tuomas/Zotero/storage/QP3WKG2G/Pennington et al_2014_Glove.pdf}
}

@phdthesis{rigutini2004,
  title = {Automatic Text Processing: {{Machine}} Learning Techniques},
  author = {Rigutini, Leonardo and Maggini, Marco},
  year = {2004},
  file = {/Users/tuomas/Zotero/storage/8WGZHUFA/Ingegneria et al_Acknowledgements.pdf;/Users/tuomas/Zotero/storage/UAUMJSLG/summary.html},
  school = {Citeseer}
}

@article{rish,
  title = {An Empirical Study of the Naive {{Bayes}} Classifier},
  author = {Rish, I},
  pages = {6},
  abstract = {The naive Bayes classifier greatly simplify learning by assuming that features are independent given class. Although independence is generally a poor assumption, in practice naive Bayes often competes well with more sophisticated classifiers.},
  file = {/Users/tuomas/Zotero/storage/5EFKSIS8/Rish - An empirical study of the naive Bayes classiﬁer.pdf},
  language = {en}
}

@article{rokach2005,
  title = {Top-{{Down Induction}} of {{Decision Trees Classifiers}}\textemdash{{A Survey}}},
  author = {Rokach, L. and Maimon, O.},
  year = {2005},
  month = nov,
  volume = {35},
  pages = {476--487},
  issn = {1094-6977},
  doi = {10.1109/TSMCC.2004.843247},
  abstract = {Decision Trees are considered to be one of the most popular approaches for representing classifiers. Researchers from various disciplines such as statistics, machine learning, pattern recognition, and data mining considered the issue of growing a decision tree from available data. This paper presents an updated survey of current methods for constructing decision tree classifiers in top-down manner. The paper suggests a unified algorithmic framework for presenting these algorithms and provides profound descriptions of the various splitting criteria and pruning methodology.},
  file = {/Users/tuomas/Zotero/storage/927PNK2J/Rokach and Maimon - 2005 - Top-Down Induction of Decision Trees Classifiers—A.pdf},
  journal = {IEEE Transactions on Systems, Man and Cybernetics, Part C (Applications and Reviews)},
  language = {en},
  number = {4}
}

@article{rosenblatt1958,
  title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.},
  author = {Rosenblatt, Frank},
  year = {1958},
  volume = {65},
  pages = {386},
  publisher = {{American Psychological Association}},
  isbn = {1939-1471},
  journal = {Psychological review},
  number = {6}
}

@techreport{rumelhart1985,
  title = {Learning Internal Representations by Error Propagation},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1985},
  institution = {{California Univ San Diego La Jolla Inst for Cognitive Science}},
  file = {/Users/tuomas/Zotero/storage/L86SJ4CK/Rumelhart et al_1985_Learning internal representations by error propagation.pdf}
}

@article{sebastiani2002,
  title = {Machine Learning in Automated Text Categorization},
  author = {Sebastiani, Fabrizio},
  year = {2002},
  volume = {34},
  pages = {1--47},
  publisher = {{ACM New York, NY, USA}},
  file = {/Users/tuomas/Zotero/storage/IC9YMUY7/Sebastiani_2002_Machine learning in automated text categorization.pdf},
  isbn = {0360-0300},
  journal = {ACM computing surveys (CSUR)},
  number = {1}
}

@article{vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archivePrefix = {arXiv},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  file = {/Users/tuomas/Zotero/storage/BV7UQQCN/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/tuomas/Zotero/storage/JWMBL7S5/1706.html},
  journal = {arXiv:1706.03762 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{virtanen2019,
  title = {Multilingual Is Not Enough: {{BERT}} for {{Finnish}}},
  shorttitle = {Multilingual Is Not Enough},
  author = {Virtanen, Antti and Kanerva, Jenna and Ilo, Rami and Luoma, Jouni and Luotolahti, Juhani and Salakoski, Tapio and Ginter, Filip and Pyysalo, Sampo},
  year = {2019},
  month = dec,
  abstract = {Deep learning-based language models pretrained on large unannotated text corpora have been demonstrated to allow efficient transfer learning for natural language processing, with recent approaches such as the transformer-based BERT model advancing the state of the art across a variety of tasks. While most work on these models has focused on high-resource languages, in particular English, a number of recent efforts have introduced multilingual models that can be fine-tuned to address tasks in a large number of different languages. However, we still lack a thorough understanding of the capabilities of these models, in particular for lower-resourced languages. In this paper, we focus on Finnish and thoroughly evaluate the multilingual BERT model on a range of tasks, comparing it with a new Finnish BERT model trained from scratch. The new language-specific model is shown to systematically and clearly outperform the multilingual. While the multilingual model largely fails to reach the performance of previously proposed methods, the custom Finnish BERT model establishes new state-of-the-art results on all corpora for all reference tasks: part-of-speech tagging, named entity recognition, and dependency parsing. We release the model and all related resources created for this study with open licenses at https://turkunlp.org/finbert .},
  archivePrefix = {arXiv},
  eprint = {1912.07076},
  eprinttype = {arxiv},
  file = {/Users/tuomas/Zotero/storage/RIUH58UV/Virtanen et al_2019_Multilingual is not enough.pdf;/Users/tuomas/Zotero/storage/PQ5IYXY2/1912.html},
  journal = {arXiv:1912.07076 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{wan,
  title = {Regularization of {{Neural Networks}} Using {{DropConnect}}},
  author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and LeCun, Yann and Fergus, Rob},
  pages = {9},
  abstract = {We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.},
  file = {/Users/tuomas/Zotero/storage/777ZQEEP/Wan et al. - Regularization of Neural Networks using DropConnec.pdf},
  language = {en}
}

@article{webster1992,
  title = {Tokenization as the {{Initial Phase}} in {{NLP}}},
  author = {Webster, Jonathan J. and Kit, Chunyu},
  year = {1992},
  file = {/Users/tuomas/Zotero/storage/CKSM7RDI/Webster_Kit_1992_Tokenization as the Initial Phase in NLP.pdf},
  journal = {COLING 1992 Volume 4: The 15th International Conference on Computational Linguistics}
}

@article{wu2016,
  title = {Google's Neural Machine Translation System: {{Bridging}} the Gap between Human and Machine Translation},
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus},
  year = {2016},
  file = {/Users/tuomas/Zotero/storage/LPU2WBQ6/Wu et al_2016_Google's neural machine translation system.pdf},
  journal = {arXiv preprint arXiv:1609.08144}
}

@book{yang2020,
  title = {Transfer Learning},
  author = {Yang, Qiang and Zhang, Yu and Dai, Wenyuan and Pan, Sinno Jialin},
  year = {2020},
  publisher = {{Cambridge University Press}},
  file = {/Users/tuomas/Zotero/storage/644HFWSL/Yang et al_2020_Transfer learning.pdf},
  isbn = {1-107-01690-8}
}

@article{young2018,
  title = {Recent {{Trends}} in {{Deep Learning Based Natural Language Processing}}},
  author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
  year = {2018},
  month = nov,
  abstract = {Deep learning methods employ multiple processing layers to learn hierarchical representations of data and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.},
  archivePrefix = {arXiv},
  eprint = {1708.02709},
  eprinttype = {arxiv},
  file = {/Users/tuomas/Zotero/storage/F4LV6I36/Young et al_2018_Recent Trends in Deep Learning Based Natural Language Processing.pdf;/Users/tuomas/Zotero/storage/FJVAAWXT/1708.html},
  journal = {arXiv:1708.02709 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}


