\chapter{Medical report document classification} \label{Medical report document classification}

\section{Problem} \label{Problem}

-- Doctors write medical reports and define the "label" aka diagnosis code for it -> might get it wrong or not attach one at all -> automatic classification would be useful. --

A model for classifying medical reports could be used to automatically define a proper diagnosis code for a new piece of text or to find older mislabelled texts.
The motivation for this was to, firstly, define the feasability and current state of deep learning for automatic text classification of medical text and, secondly, to build a set of tools for the hospital in order for them to be able to train classifiers for different diagnosis codes.

Since compute resources was a major constraint, only approaches with a reasonable single-gpu training time were considered.
Thus, ULMFiT, ELECTRA, and FinBERT, pretrained by Virtanen et al. \cite{virtanen2019}, were chosen for comparison.
In addition to comparing the different models with each other, each model - excluding FinBERT - was trained multiple times with different vocabulary sizes to see if it impacted the results.
Word tokens were extracted with SentencePiece after the data had been preprocessed.

\section{Data} \label{Data}
\subsection{Medical reports}\label{Medical reports}
The medical data for the work was provided by (VSSHP/Auria/TYKS).
It consists of doctor's statements with corresponding diagnosis codes and other metadata, such as date of visit and health center office, of patients in the Turku-region.
No metadata was utilized in the training data, only the written text and the corresponding diagnosis code.
Since the actual diagnosis code for the text appeared often in it, all diagnosis codes in text were masked with a [CODE] token in the training data to prevent the model from learning from them.
HTML-tags such as <br> were removed in preprocessing and texts that were deemed too short (less than 80 characters) were also removed from the training set.
After preprocessing, the data consisted of X tokens in Y sentences.
-- GET DATA FOR CLINICAL, TOKENS AND SENTENCES --

\subsection{General Finnish}\label{General Finnish}
Since receiving access to the actual medical data took some time, the chosen models and code were additionally trained and tested on a general finnish corpus that Virtanen et al. compiled for FinBERT \cite{virtanen2019}.
It is composed from three different sources: news articles, online discussions, and documents crawled from the Finnish internet, and consists of 3.3 billion tokens from 234 million sentences.
The total size of the corpus is roughly 30 times the size of the Finnish Wikipedia.
The corpus was extensively preprocessed by filtering out documents that had too high a ratio of digits, uppercase or non-Finnish alphabetic characters, or low average sentence length.
Additionally, documents that featured 25\% or more duplication were removed as well as heuristically defined undesireables \cite{virtanen2019}.


\section{Compute resources} \label{Compute resources}
Compute resources for the project were provided by (VSSHP/Auria/Tyks) for the medical models, and CSC for the general finnish models.

\subsection{CSC Puhti} \label{CSC Puhti}
CSC (IT Center for Science Ltd.) is a non-profit state enterprise owned by the Finnish state and higher education institutions in Finland.
It offers compute resources for scientific purposes to universities and upkeeps the FUNET network, which is the Finnish national research and education network.
CSC operates two supercomputers, namely Taito and Puhti, and is working on a new supercomputer, Mahti, that is scheduled to open for use some time in 2020.
For this project, Puhti was chosen since it provides an "artificial intelligence partition" with access to GPU nodes with multiple Nvidia V100 graphics cards.

\subsection{VSSHP Blackbird} \label{VSSHP Blackbird}
For training models on the clinical data, access was granted to Blackbird, a computer for artificial intelligence owned by (AURIA/TYKS/VSSHP?) with four Nvidia V100 graphics cards allowing simultaneous training of multiple models.
Although the architecture could have managed training a medium-sized BERT, it was considered too long of a task to reserve the compute resources for.


\section{Results} \label{Results}
A binary classifier was built to identify texts that had one of two knee-related diagnosis codes from the training data.

-- RESULT TABLE WITH: --
-- Acc, precision, f1 --
-- FOR:--
-- ULMFIT 30K, 50K, 100K --
-- ELECTRA 30K, 50K, 100K --
-- FinBERT with original vocab of 50K --

-- TABLE OF TRAINING TIMES FOR DIFFERENT MODELS, PRETRAINING AND FINETUNING, OMIT FINBERT PRETRAINING TIME SINCE ALREADY PRETRAINED --

\section{Discussion}\label{Discussion}
Even though ELECTRA required more than twice the compute than ULMFiT, it performed worse.
This could be due to the rather limited amount of clinical data they were trained on.

-- SURPRISING RESULTS FROM ULMFIT WRT THE PERFORMANCE OF ELECTRA --
-- FINBERT IS OKAY THOUGH MODELS TRAINED ON THE ACTUAL DATA PERFORM BETTER --
FinBERT
-- ELECTRA SEEMS PROMISING --
-- FUTURE WORK COULD INCLUDE PRETRAINING ON A LARGER CORPUS OF GENERAL MEDICAL TEXT FIRST, THEN FINETUNE TO TASK --
For future work, pretraining ELECTRA or even BERT on a larger, general medical text corpus before finetuning on domain data could yield overall improvements to diagnosis code classifiers.
Additionally, comparing the performance of the models with different amounts of training data would give insight into the limits of ULMFiT and ELECTRA.
asdf
