\chapter{Medical report document classification} \label{Medical report document classification}

\section{Problem definition} \label{Problem definition}

-- Doctors write medical reports and define the ``label'' aka diagnosis code for it -> might get it wrong or not attach one at all -> automatic classification would be useful. --


Since compute resources was a major constraint, only approaches with a reasonable single-gpu training time were considered.
Thus, ULMFiT, ELECTRA, and FinBERT which was pretrained by Virtanen et al.~\cite{virtanen2019}, were chosen for comparison.
In addition to comparing the different models with each other, each model --- excluding FinBERT --- was trained multiple times with different vocabulary sizes to see if it impacted the results.
Word tokens were extracted with SentencePiece after the data had been preprocessed.

\section{Data} \label{Data}
\subsection{Medical reports}\label{Medical reports}
The medical data for the work was provided by Auria services at the Turku university hospital.
It consists of doctor's statements with corresponding diagnosis codes and other metadata, such as date of visit and health center office, of patients in the Turku-region.
No metadata was utilized in the training data, only the written text and the corresponding diagnosis code.
Since the actual diagnosis code for the text appeared often in it, all diagnosis codes in text were masked with a [CODE] token in the training data to prevent the model from learning from them.
HTML-tags such as <br> were removed in preprocessing and texts that were deemed too short (less than 80 characters) were also removed from the training set.
After preprocessing, the data consisted of X tokens in Y sentences.

Class balance for the dataset was 85\% negative and 15\% positive samples.
The dataset was eventually balanced by upsampling the positive samples.
-- GET DATA FOR CLINICAL, TOKENS AND SENTENCES --

\subsection{General Finnish}\label{General Finnish}
Since receiving access to the actual medical data took some time, the chosen models and code were additionally trained and tested on a general finnish corpus that Virtanen et al. compiled for FinBERT \cite{virtanen2019}.
It is composed from three different sources: news articles, online discussions, and documents crawled from the Finnish internet, and consists of 3.3 billion tokens from 234 million sentences.
The total size of the corpus is roughly 30 times the size of the Finnish Wikipedia.
The corpus was extensively preprocessed by filtering out documents that had too high a ratio of digits, uppercase or non-Finnish alphabetic characters, or low average sentence length.
Additionally, documents that featured 25\% or more duplication were removed as well as heuristically defined undesirables~\cite{virtanen2019}.


\section{Compute resources} \label{Compute resources}
Compute resources for the project were provided by Auria services for the medical models, and CSC for the general Finnish models.

\subsection{CSC} \label{CSC}
CSC (IT Center for Science Ltd.) is a non-profit state enterprise owned by the Finnish state and higher education institutions in Finland.
It offers compute resources for scientific purposes to universities and upkeeps the FUNET network, which is the Finnish national research and education network.
CSC operates two supercomputers, namely Taito and Puhti, and is working on a new supercomputer, Mahti, that is scheduled to open for use some time in 2020.
For this project, Puhti was chosen since it provides an ``artificial intelligence partition'' with access to GPU nodes with multiple Nvidia V100 graphics cards.

\subsubsection{Puhti}\label{Puhti}
Puhti was launched on September 2, 2019.
It is an Atos cluster system and has a variety of different node types.

Puhti has 682 CPU nodes, with a theoretical peak performance of 1,8 petaflops, and an AI partition of 80 GPU nodes with a peak performance of 2,7 petaflops.
Each node is equipped with two Intel Xeon processors, code name Cascade Lake, with 20 cores each running at 2,1 GHz.
Each GPU node also has four Nvidia Volta V100 GPUs with 32 GB of memory each.
The nodes are equipped with 384 GB of main memory and 3,6 TB of fast local storage.
The AI partition is engineered to allow GPU-intensive workloads to scale well across multiple nodes~\cite{zotero-178}.

When working with Puhti on this project, the workflow consisted of coding and testing the neural networks locally first, and then using Slurm to run batch jobs on Puhti.
This lead to some additional overhead in time for the project since working simultaneously on two environments proved quite arduous.
Additionally, keeping tabs on the versions of code was very important since it could be altered in both locations, thus git \cite{zotero-186} was used for this version control.

\subsubsection{Slurm}\label{Slurm}
Puhti uses the Slurm workload manager \cite{zotero-174} to handle scheduling jobs for compute clusters.
It is an open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for Linux clusters.
First, it manages the allocation of exclusive and/or non-exclusive access to compute nodes to users for some duration of time during which they can perform work.
Second, it provides a framework for starting, executing, and monitoring work on the set of allocated nodes.
Finally, it manages a queue of pending work to arbitrate the contention for resources~\cite{zotero-176}.


\subsection{VSSHP Blackbird} \label{VSSHP Blackbird}
For training models on the clinical data, access was granted to Blackbird, a computer for artificial intelligence owned by (AURIA/TYKS/VSSHP?) with four Nvidia V100 graphics cards allowing simultaneous training of multiple models.
Although the architecture could have managed training a medium-sized BERT, it was considered too long of a task to reserve the compute resources for.

The workflow on Blackbird consisted of ssh'ing to the linux-based computer, using screen~\cite{zotero-180} to multiplex the connection to multiple shells, and running a training process on each shell.
Jupyter notebooks~\cite{zotero-182} were also used on the machine for data visualization and prototyping, and were accessed locally by using ssh tunneling~\cite{zotero-184}.

\subsection{Methods}\label{Methods}
\subsubsection{ULMFiT}\label{ULMFiT}
The fastai v1 -library was used for implementing ULMFiT\footnote{https://github.com/fastai/fastai}.
As the library's support for SentencePiece\footnote{https://github.com/google/sentencepiece} was at the time quite limited, a considerable amount of custom code had to be written to incorporate the sub-word tokenizer in the training process.
The scripts used for training ULMFiT with SentencePiece are open-sourced and can be found from\footnote{https://github.com/invisiblesheep/ulmfit-sentencepiece}.

\subsubsection{ELECTRA}\label{ELECTRA}
For training ELECTRA, the pretraining and finetuning scripts were used from the official github repository\footnote{https://github.com/google-research/electra}.
The code was forked in order to make some changes to it regarding finetuning and evaluation parameters\footnote{https://github.com/invisiblesheep/electra}.

\subsubsection{BERT}\label{BERT}
The code for finetuning FinBERT was taken from the official BERT github repository\footnote{https://github.com/google-research/bert}, and FinBERT itself can be found from\footnote{https://github.com/TurkuNLP/FinBERT}.

\subsubsection{fastText}\label{fastText}
fastText\footnote{https://github.com/facebookresearch/fastText} was trained as a baseline model.
In addition to providing word embeddings, fastText can be used as a classifier as well.
fastText obtains document vectors by averaging word embeddings after which it uses multinomial logistic regression for classification.
A probability distribution over classes is gained as a result after applying the softmax function to the results.
It uses a bunch of tricks, such as hierarchical softmax, to up the speed of training the model.
Thus it's an order of magnitude faster to train than a deep learning model but it still is somewhat competitive with one.

\section{Results}\label{Results}
A binary classifier was built to identify texts that had one of two knee-related diagnosis codes from the training data.

\begin{table}[t]
\begin{center}
\begin{tabular}{lccc}
         & Precision & Recall & F1 \\
        \hline
FinBERT &      ---   &    --- &  ---   \\
ELECTRA-30K    &     71.90 & 61.50 & 66.29 \\
ELECTRA-50K  &      70.65 & 74.38 & 72.47 \\
ELECTRA-100K  &     72.60 & 65.27 & 68.74 \\
ULMFiT-30K  &    84.28 & 72.54 & 77.97 \\
ULMFiT-50K  &   82.91 & 75.41 & 78.98 \\
ULMFiT-100K  &  83.95 & 75.17 & \textbf{79.32} \\
fastText &  84.34 & 70.07 & 76.54 \\
\end{tabular}
\caption{Classification results of models with different vocabulary sizes on evaluation set.}
\label{table:results}
\end{center}
\end{table}

fastText's results were achieved by first running it's hyperparameter optimization command to find well-performing parameters after which the final parameters were forked manually.
The final hyperparameters were a learning rate of 0.05, 25 epochs of training and the usage of word bigrams.

Mainly default hyperparameters were used while training ULMFiT, except the learning rate was found with fast.ai's learning rate finder.
The underlying AWD-LSTM -architecture was not changed in any way.
ULMFiT was pretrained and finetuned for 5 and 12 epochs, respectively.
The finetuning results stopped improving after 8 finetuning epochs.


\section{Discussion}\label{Discussion}
Class imbalance in the training data proved to be a factor for model performance.
Upsampling by random duplication of positive samples was used to balance the classes, and it seemed to somewhat improve performance.

Even though ELECTRA required more than twice the compute than ULMFiT, it performed worse.
This could be due to the rather limited size of the clinical training data.

-- SURPRISING RESULTS FROM ULMFIT WRT THE PERFORMANCE OF ELECTRA --
-- FINBERT IS OKAY THOUGH MODELS TRAINED ON THE ACTUAL DATA PERFORM BETTER --
Unfortunately, training FinBERT proved difficult as the finetuned models failed to learn to predict the training data.
The finetuned model would, depending on hyperparameters, predict only either all positives or all negatives.

-- ELECTRA SEEMS PROMISING --
-- FUTURE WORK COULD INCLUDE PRETRAINING ON A LARGER CORPUS OF GENERAL MEDICAL TEXT FIRST, THEN FINETUNE TO TASK --
For future work, pretraining ELECTRA or even BERT on a larger, general medical text corpus before finetuning on domain data could yield overall improvements to diagnosis code classifiers.
Additionally, comparing the performance of the models with different amounts of training data would give insight into the limits of ULMFiT and ELECTRA.
